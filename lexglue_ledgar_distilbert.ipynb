{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYsuXQWp/5up0GQsqx6Nt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmanabbasi36/legal_agent/blob/main/lexglue_ledgar_distilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv9u3fD2hqNI"
      },
      "outputs": [],
      "source": [
        "# Colab cell 1: runtime = GPU\n",
        "!pip install -q transformers datasets accelerate evaluate[sentencepiece] scikit-learn huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # follow prompts"
      ],
      "metadata": {
        "id": "IkoEtwJuh8Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "E5sLllL6ih1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"coastalcph/lex_glue\"\n",
        "DATA_CONFIG = \"ledgar\"\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = '/content/lexglue-ledgar-distilbert'"
      ],
      "metadata": {
        "id": "3xVYUt-tjmY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(DATASET, DATA_CONFIG)\n",
        "print(ds)\n",
        "\n",
        "print(ds['train'][0])"
      ],
      "metadata": {
        "id": "PzCezvLOkqi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Inspect features to find text field & label field\n",
        "features = ds['train'].features\n",
        "print(\"Features keys:\", features)\n",
        "\n",
        "# heuristics for text column:\n",
        "possible_text_cols = ['text', 'sentence', 'prompt', 'excerpt', 'passage']\n",
        "text_col = None\n",
        "for col in possible_text_cols:\n",
        "    if col in ds['train'].column_names:\n",
        "        text_col = col\n",
        "        break\n",
        "# fallback: choose first string column\n",
        "if text_col is None:\n",
        "    for k, v in features.items():\n",
        "        if str(v).startswith('Value') and k != 'label':\n",
        "            text_col = k\n",
        "            break\n",
        "\n",
        "print(\"Using text column:\", text_col)\n",
        "\n",
        "# label column detection\n",
        "label_col = None\n",
        "for c in ['label', 'labels', 'labels_ids', 'gold_label']:\n",
        "    if c in ds['train'].column_names:\n",
        "        label_col = c\n",
        "        break\n",
        "# fallback: pick first int/class column\n",
        "if label_col is None:\n",
        "    for k, v in features.items():\n",
        "        if \"ClassLabel\" in str(v) or \"Sequence\" in str(v):\n",
        "            label_col = k\n",
        "            break\n",
        "print(\"Using label column:\", label_col)\n",
        "\n",
        "# prepare labels\n",
        "label_feature = features[label_col]\n",
        "if \"ClassLabel\" in str(label_feature):\n",
        "    id2label = label_feature.names\n",
        "    label2id = {l:i for i,l in enumerate(id2label)}\n",
        "    num_labels = len(id2label)\n",
        "    multi_label = False\n",
        "else:\n",
        "    # Could be multi-hot or list -> handle as multi-label\n",
        "    # Convert lists to binary vectors later\n",
        "    # We'll infer at tokenization time\n",
        "    id2label = None\n",
        "    label2id = None\n",
        "    num_labels = None\n",
        "    multi_label = True\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess(examples):\n",
        "    texts = examples[text_col]\n",
        "    enc = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
        "    # labels\n",
        "    labs = examples[label_col]\n",
        "    if not multi_label and id2label is not None:\n",
        "        enc[\"labels\"] = [label for label in labs]\n",
        "    else:\n",
        "        # if labels are lists of ints (multi-label)\n",
        "        # create binary vector of length K (we can compute K from dataset)\n",
        "        # compute num_labels if unknown\n",
        "        global num_labels\n",
        "        if num_labels is None:\n",
        "            # estimate max label id +1\n",
        "            maxid = 0\n",
        "            for l in ds['train'][label_col]:\n",
        "                if isinstance(l, list):\n",
        "                    if l:\n",
        "                        maxid = max(maxid, max(l))\n",
        "            num_labels = maxid + 1\n",
        "        enc[\"labels\"] = [ [1 if i in lab else 0 for i in range(num_labels)] for lab in labs ]\n",
        "    return enc\n",
        "\n",
        "tokenized = ds.map(preprocess, batched=True, remove_columns=ds['train'].column_names)\n",
        "tokenized = tokenized.shuffle(seed=42)\n",
        "tokenized\n"
      ],
      "metadata": {
        "id": "Gx1gC06FoRvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "if not multi_label:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "else:\n",
        "    # For multi-label classification, set problem_type so Trainer uses BCEWithLogitsLoss\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "quTJyonFoi1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    if multi_label:\n",
        "        preds = (logits > 0).astype(int)  # logits already converted by trainer to numpy\n",
        "        # compute micro/macro f1\n",
        "        return {\n",
        "            \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
        "            \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "        }\n",
        "    else:\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
        "        return {\"accuracy\": acc, \"f1_macro\": f1_macro}\n"
      ],
      "metadata": {
        "id": "_3F7jB6qoz1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\" if not multi_label else \"f1_micro\",\n",
        "    fp16=True,  # use mixed precision if supported\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"] if \"validation\" in tokenized else tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "h11yOTzYo5xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "OoOJHKAdpGPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n",
        "\n",
        "# Save locally / to Drive\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved to\", OUTPUT_DIR)\n",
        "\n",
        "# Optionally push to Hub\n",
        "# trainer.push_to_hub(\"my-lexglue-ledgar-distilbert\")\n"
      ],
      "metadata": {
        "id": "EXSB3mc-stsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()   # it will ask: \"Enter your token\"\n"
      ],
      "metadata": {
        "id": "hZzWYn-4_yTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"SalmanAbbasi/lexglue-ledgar-distilbert\"\n",
        "model.push_to_hub(model_name)\n",
        "tokenizer.push_to_hub(model_name)\n"
      ],
      "metadata": {
        "id": "Dp8Cv_jx368Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-classification\", model=\"/content/lexglue-ledgar-distilbert\", tokenizer=\"/content/lexglue-ledgar-distilbert\")\n",
        "\n",
        "text = \"This clause sets out the obligations of each party regarding confidentiality.\"\n",
        "print(pipe(text))\n"
      ],
      "metadata": {
        "id": "KhLDeHsEAvgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"coastalcph/lex_glue\", \"ledgar\")\n",
        "label_names = ds['train'].features['label'].names\n",
        "print(label_names[:10])  # show first few labels\n",
        "\n",
        "# Now decode your prediction\n",
        "result = pipe(text)[0]\n",
        "label_id = int(result['label'].split('_')[-1])\n",
        "print(f\"Predicted label: {label_names[label_id]} ({result['score']:.2%} confidence)\")\n"
      ],
      "metadata": {
        "id": "Y8EOmfXFA4Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update config with id2label and label2id\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(\"/content/lexglue-ledgar-distilbert\")\n",
        "config.id2label = {i: name for i, name in enumerate(label_names)}\n",
        "config.label2id = {name: i for i, name in enumerate(label_names)}\n",
        "config.save_pretrained(\"/content/lexglue-ledgar-distilbert\")\n"
      ],
      "metadata": {
        "id": "dAaFUVSbBlsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMQ7OUz4CPxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}